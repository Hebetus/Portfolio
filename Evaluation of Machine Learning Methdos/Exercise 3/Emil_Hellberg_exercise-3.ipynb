{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TKO_7092 Evaluation of Machine Learning Methods 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Student name: Emil Hellberg\n",
    "\n",
    "Student number: 1901299\n",
    "\n",
    "Student email: ephell@utu.fi\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Complete the tasks given to you in the letter below. In your submission, explain clearly, precisely, and comprehensively why the cross-validation described in the letter failed, what is the correct way to perform cross-validation in the given scenario, and why the correct cross-validation method will give a reliable estimate of the generalisation performance. Then implement the correct cross-validation for the scenario and report its results.\n",
    "\n",
    "Remember to follow all the general exercise guidelines that are stated in Moodle. Full points (2p) will be given for a submission that demonstrates a deep understanding of cross-validation on pair-input data and implements the requested cross-validation correctly (incl. reporting the results). Partial points (1p) will be given if there are small error(s) but the overall approach is correct. No points will be given if there are significant error(s).\n",
    "\n",
    "The deadline of this exercise is **Wednesday 19 February 2025 at 11:59 PM**. Please contact Juho Heimonen (juaheim@utu.fi) if you have any questions about this exercise.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dear Data Scientist,\n",
    "\n",
    "I have a long-term research project regarding a specific set of proteins. I am attempting to discover small organic compounds that can bind strongly to these proteins and thus act as drugs. I have already made laboratory experiments to measure the affinities between some proteins and drug molecules.\n",
    "\n",
    "My colleague is working on another set of proteins, and the objectives of his project are similar to mine. He has recently discovered thousands of new potential drug molecules. He asked me if I could find the pairs that have the strongest affinities among his proteins and drug molecules. Obviously I do not have the resources to measure all the possible pairs in my laboratory, so I need to prioritise. I decided to do this with the help of machine learning, but I have encountered a problem.\n",
    "\n",
    "Here is what I have done so far: First I trained a K-nearest neighbours regressor with the parameter value K=10 using all the 400 measurements I had already made in the laboratory with my proteins and drug molecules. They comprise of 77 target proteins and 59 drug molecules. Then I performed a leave-one-out cross-validation with this same data to estimate the generalisation performance of the model. I used C-index and got a stellar score above 90%. Finally I used the model to predict the affinities of my colleague's proteins and drug molecules. The problem is: when I selected the highest predicted affinities and tried to verify them in the lab, I found that many of them are much lower in reality. My model clearly does not work despite the high cross-validation score.\n",
    "\n",
    "Please explain why my estimation failed and how leave-one-out cross-validation should be performed to get a reliable estimate. Also, implement the correct leave-one-out cross-validation and report its results. I need to know whether it would be a waste of my resources if I were to use my model any further.\n",
    "\n",
    "The data I used to create my model is available in the files `input.data`, `output.data` and `pairs.data` for you to use. The first file contains the features of the pairs, whereas the second contains their affinities. The third file contains the identifiers of the drug and target molecules of which the pairs are composed. The files are paired, i.e. the i<sup>*th*</sup> row in each file is about the same pair.\n",
    "\n",
    "Looking forward to hearing from you soon.\n",
    "\n",
    "Yours sincerely, \\\n",
    "Bio Scientist\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer the questions about cross-validation on pair-input data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Why did the estimation described in the letter fail?\n",
    "# How should leave-one-out cross-validation be performed in the given scenario and why?\n",
    "# Remember to provide comprehensive and precise arguments.\n",
    "\n",
    "The training and testing described in the letter don't resemble leave-one-out cv since the there was no train/test split for the dataset used. This causes the model to be trained and tested with all data, which initially tends to give a great performance value, but will likely fail in regression/classification when used with new data. This can be seen as evidence that the model trained in this way incorporates noise (all the noise since all the data is used) which in turn decreases the models ability to generalize, i.e. generate knowledge.\n",
    "\n",
    "The data contains pair-input data (measurements were made from combinations of target proteins and potential drugs), which means the input variables in input.data could concern either member of these pairs. The output describes the pairs' affinity. Any target protein can bind with any drug molecule. Closer inspection on the data reveals that each of the 400 rows in the pairs.data is a unique combination, with the same drug molecules tested on multiple different target proteins.\n",
    "\n",
    "The proper way to perform leave-one-out cross validation for this dataset is to iterate over the whole dataset, training the model on all but one of the input rows on each iteration, and then at the end of the iteration to perform a regression/classification test with the 1 remaining input row that wasn't used for model training, and then measuring the error on that one input. Finally after iterating the whole dataset you compute the mean of all these errors (or whatever metric/unit is used fo evaluation) to represent the statistical performance of the given model, with chosen hyperparameters (In this case k = 10) on the dataset under study. Since the data consists of pair-input data, leave-one-out cross validation needs do be slightly modified, to get 4 different performance evaluation metrics that resemble the possible categories of future data. This means that instead of training the model at each iteration once, it must be trained and evaluated with 4 permutations of the training fold (both members shared, first member shared, second member shared, no members shared between training instances and testing data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries you need.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the utility functions you need in your analysis.\n",
    "# Input data seems to already be normalized and pre-processed\n",
    "\n",
    "# Returns data points that share the second member but not the first\n",
    "\n",
    "def B(inputs, n, pairs):\n",
    "    B_inputs = pd.DataFrame(columns = inputs.columns)\n",
    "    B_inputs.loc[len(B_inputs)] = 0\n",
    "    \n",
    "    for i in range(0, len(inputs)):\n",
    "        if pairs.iloc[i, 0] != pairs.iloc[n, 0] and pairs.iloc[i, 1] == pairs.iloc[n, 1]:\n",
    "            new_row = inputs.iloc[[i]]\n",
    "            B_inputs = pd.concat([B_inputs, new_row], ignore_index = True)\n",
    "\n",
    "    return B_inputs\n",
    "\n",
    "# Returns data points that share the first mebmer but not the second\n",
    "\n",
    "def C(inputs, n, pairs):\n",
    "    C_inputs = pd.DataFrame(columns = inputs.columns)\n",
    "    C_inputs.loc[len(C_inputs)] = 0\n",
    "    \n",
    "    for i in range(0, len(inputs)):\n",
    "        if pairs.iloc[i, 1] != pairs.iloc[n, 1] and pairs.iloc[i, 0] == pairs.iloc[n, 0]:\n",
    "            new_row = inputs.iloc[[i]]\n",
    "            C_inputs = pd.concat([C_inputs, new_row], ignore_index = True)\n",
    "\n",
    "    return C_inputs\n",
    "\n",
    "# Returns data points that share neither member\n",
    "\n",
    "def D(inputs, n, pairs):\n",
    "    D_inputs = pd.DataFrame(columns = inputs.columns)\n",
    "    D_inputs.loc[len(D_inputs)] = 0\n",
    "    for i in range(0, len(inputs)):\n",
    "        if pairs.iloc[i, 0] != pairs.iloc[n, 0] and pairs.iloc[i, 1] != pairs.iloc[n, 1]:\n",
    "            new_row = inputs.iloc[[i]]\n",
    "            D_inputs = pd.concat([D_inputs, new_row], ignore_index = True)\n",
    "\n",
    "    return D_inputs\n",
    "\n",
    "def cindex(y, yp):\n",
    "    n = 0\n",
    "    h_num = 0 \n",
    "    \n",
    "    for i in range(0, len(y)):\n",
    "        t = y[i]\n",
    "        p = yp[i]\n",
    "        for j in range(i + 1, len(y)):\n",
    "            nt = y[j]\n",
    "            np = yp[j]\n",
    "            if (t != nt):\n",
    "                n = n + 1\n",
    "                if (p < np and t < nt) or (p > np and t > nt): \n",
    "                    h_num += 1\n",
    "                elif (p == np):\n",
    "                    h_num += 0.5\n",
    "    return h_num / n    \n",
    "\n",
    "def scross_validate(x, y, pairs):\n",
    "    columns = ['A', 'B', 'C', 'D']\n",
    "    c_index = pd.DataFrame(columns = columns)\n",
    "\n",
    "    y_true = []\n",
    "    A_predictions = []\n",
    "    B_predictions = []\n",
    "    C_predictions = []\n",
    "    D_predictions = []\n",
    "\n",
    "    for n in range(0, len(x)):\n",
    "        test_x = x.iloc[n]\n",
    "        true_y = y.iloc[n, 0]\n",
    "\n",
    "        A_train_x = x.drop(x.index[n])\n",
    "        A_train_y = y.drop(y.index[n])\n",
    "        B_train_x = B(x, n, pairs)\n",
    "        B_train_y = B(y, n, pairs)\n",
    "        C_train_x = C(x, n, pairs)\n",
    "        C_train_y = C(y, n, pairs)\n",
    "        D_train_x = D(x, n, pairs)\n",
    "        D_train_y = D(y, n, pairs)\n",
    "\n",
    "        neigh_A = KNeighborsRegressor(n_neighbors = 10)\n",
    "        neigh_A.fit(A_train_x, A_train_y)\n",
    "        A_pred = neigh_A.predict(test_x.to_numpy().reshape(1, -1))\n",
    "        A_predictions.append(A_pred)\n",
    "        \n",
    "        neigh_B = KNeighborsRegressor(n_neighbors = 10)\n",
    "        if B_train_x.shape[0] > 10:\n",
    "            neigh_B.fit(B_train_x, B_train_y)\n",
    "            B_pred = neigh_B.predict(test_x.to_numpy().reshape(1, -1))\n",
    "            B_predictions.append(B_pred)\n",
    "        else:\n",
    "            B_predictions.append(0.5)\n",
    "        \n",
    "        neigh_C = KNeighborsRegressor(n_neighbors = 10)\n",
    "        if C_train_x.shape[0] > 10:\n",
    "            neigh_C.fit(C_train_x, C_train_y)\n",
    "            C_pred = neigh_C.predict(test_x.to_numpy().reshape(1, -1))\n",
    "            C_predictions.append(C_pred)\n",
    "        else:\n",
    "            C_predictions.append(0.5)\n",
    "        \n",
    "        neigh_D = KNeighborsRegressor(n_neighbors = 10)\n",
    "        if D_train_x.shape[0] > 10:\n",
    "            neigh_D.fit(D_train_x, D_train_y)\n",
    "            D_pred = neigh_D.predict(test_x.to_numpy().reshape(1, -1))\n",
    "            D_predictions.append(D_pred)\n",
    "        else:\n",
    "            D_predictions.append(0.5)\n",
    "\n",
    "        y_true.append(true_y)\n",
    "    print(A_predictions)\n",
    "    print(B_predictions)\n",
    "    print(C_predictions)\n",
    "    print(D_predictions)\n",
    "    A_cindex = cindex(A_predictions, y_true)\n",
    "    B_cindex = cindex(B_predictions, y_true)\n",
    "    C_cindex = cindex(C_predictions, y_true)\n",
    "    D_cindex = cindex(D_predictions, y_true)\n",
    "    new_row_df = pd.DataFrame({'A': [A_cindex], 'B': [B_cindex], 'C': [C_cindex], 'D': [D_cindex]})\n",
    "    c_index = pd.concat([c_index, new_row_df], ignore_index = True)\n",
    "    \n",
    "    return c_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0         1         2         3         4         5         6   \\\n",
      "0    0.759222  0.709585  0.253151  0.421082  0.727780  0.404487  0.709027   \n",
      "1    0.034584  0.304720  0.688257  0.296396  0.151878  0.830755  0.270656   \n",
      "2    0.737867  0.236079  0.905987  0.163612  0.801455  0.789823  0.393999   \n",
      "3    0.406913  0.607740  0.235365  0.888679  0.150347  0.598991  0.130108   \n",
      "4    0.697707  0.432565  0.650329  0.886065  0.328660  0.576926  0.523100   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "395  0.496498  0.454389  0.353502  0.696922  0.876419  0.379429  0.733514   \n",
      "396  0.188616  0.554824  0.609247  0.371482  0.588356  0.667919  0.297278   \n",
      "397  0.095054  0.452918  0.942931  0.576332  0.411317  0.561792  0.837251   \n",
      "398  0.166109  0.471535  0.509825  0.415422  0.620681  0.786712  0.150722   \n",
      "399  0.817295  0.326707  0.500573  0.022480  0.266418  0.463136  0.720725   \n",
      "\n",
      "           7         8         9   ...        57        58        59  \\\n",
      "0    0.242963  0.407292  0.379971  ...  0.838616  0.165050  0.515334   \n",
      "1    0.705392  0.186120  0.085594  ...  0.472762  0.730013  0.639373   \n",
      "2    0.522067  0.411352  0.781861  ...  0.595468  0.582292  0.836193   \n",
      "3    0.465818  0.799953  0.906878  ...  0.453880  0.311799  0.534668   \n",
      "4    0.080463  0.131349  0.913496  ...  0.583892  0.444141  0.249423   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "395  0.839360  0.212366  0.530528  ...  0.054742  0.212866  0.657035   \n",
      "396  0.269298  0.856952  0.697523  ...  0.549288  0.843150  0.739872   \n",
      "397  0.806083  0.581221  0.829656  ...  0.900376  0.378294  0.360243   \n",
      "398  0.282159  0.809963  0.809090  ...  0.587865  0.955456  0.714566   \n",
      "399  0.900571  0.178126  0.405540  ...  0.711878  0.362695  0.555318   \n",
      "\n",
      "           60        61        62        63        64        65        66  \n",
      "0    0.332678  0.577533  0.678125  0.463608  0.538938  0.460883  0.345251  \n",
      "1    0.445218  0.455680  0.090737  0.308432  0.079023  0.603089  0.197008  \n",
      "2    0.281514  0.791790  0.081695  0.583450  0.422539  0.076437  0.299662  \n",
      "3    0.563793  0.727767  0.172686  0.908368  0.786892  0.790459  0.666388  \n",
      "4    0.110690  0.420770  0.250148  0.196350  0.427255  0.166715  0.919720  \n",
      "..        ...       ...       ...       ...       ...       ...       ...  \n",
      "395  0.483128  0.807080  0.566457  0.379042  0.566572  0.512170  0.421929  \n",
      "396  0.481870  0.359285  0.593446  0.788714  0.142521  0.819989  0.637718  \n",
      "397  0.259965  0.716623  0.817797  0.792314  0.736228  0.233031  0.597934  \n",
      "398  0.520387  0.397173  0.575056  0.822135  0.096667  0.946201  0.604271  \n",
      "399  0.145665  0.309317  0.884572  0.332518  0.944396  0.070304  0.052850  \n",
      "\n",
      "[400 rows x 67 columns]\n",
      "            0\n",
      "0    0.733933\n",
      "1    0.569419\n",
      "2    0.832588\n",
      "3    0.389664\n",
      "4    0.725953\n",
      "..        ...\n",
      "395  0.355154\n",
      "396  0.331555\n",
      "397  0.098822\n",
      "398  0.393137\n",
      "399  0.911268\n",
      "\n",
      "[400 rows x 1 columns]\n",
      "       0    1\n",
      "0    D40   T2\n",
      "1    D31  T64\n",
      "2     D6  T58\n",
      "3    D56  T49\n",
      "4    D20  T28\n",
      "..   ...  ...\n",
      "395  D30  T27\n",
      "396  D53  T11\n",
      "397  D29  T27\n",
      "398  D53  T50\n",
      "399   D4  T15\n",
      "\n",
      "[400 rows x 2 columns]\n",
      "0   1  \n",
      "D7  T25    1\n",
      "    T21    1\n",
      "    T2     1\n",
      "D6  T58    1\n",
      "    T52    1\n",
      "          ..\n",
      "D1  T39    1\n",
      "    T29    1\n",
      "    T20    1\n",
      "    T16    1\n",
      "    T10    1\n",
      "Name: count, Length: 400, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read the data files (input.data, output.data, pairs.data).\n",
    "x = pd.read_csv('./input.data', header = None)\n",
    "y = pd.read_csv('./output.data', header = None)\n",
    "pairs = pd.read_csv('./pairs.data', header = None)\n",
    "\n",
    "x = x.iloc[:, 0].str.split(expand = True).astype(float)\n",
    "pairs = pairs[0].str.split(expand = True)\n",
    "pairs[1] = pairs[1].str.replace('\"', '', regex = False)\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "print(pairs)\n",
    "print(pairs.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement and run cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0.6657572]]), array([[0.4630602]]), array([[0.8235482]]), array([[0.488935]]), array([[0.6697487]]), array([[0.5514578]]), array([[0.7620587]]), array([[0.46079093]]), array([[0.6853358]]), array([[0.5665465]]), array([[0.2088952]]), array([[0.7457613]]), array([[0.6200399]]), array([[0.24197293]]), array([[0.6997526]]), array([[0.598196]]), array([[0.2431589]]), array([[0.18462497]]), array([[0.28464363]]), array([[0.7568015]]), array([[0.433913]]), array([[0.9113759]]), array([[0.5618109]]), array([[0.2114425]]), array([[0.758541]]), array([[0.4841588]]), array([[0.4795423]]), array([[0.3803126]]), array([[0.545538]]), array([[0.8933951]]), array([[0.3701456]]), array([[0.6506979]]), array([[0.8938211]]), array([[0.7585732]]), array([[0.7230043]]), array([[0.6305093]]), array([[0.6057068]]), array([[0.7560015]]), array([[0.7247979]]), array([[0.28397673]]), array([[0.8267603]]), array([[0.552607]]), array([[0.5225022]]), array([[0.3505481]]), array([[0.7416419]]), array([[0.7556979]]), array([[0.6030746]]), array([[0.7235481]]), array([[0.16283483]]), array([[0.626768]]), array([[0.4291909]]), array([[0.5153354]]), array([[0.3711527]]), array([[0.3254603]]), array([[0.5313898]]), array([[0.8292283]]), array([[0.8190885]]), array([[0.8360681]]), array([[0.6308198]]), array([[0.4942884]]), array([[0.6312597]]), array([[0.7565058]]), array([[0.637109]]), array([[0.593322]]), array([[0.4916357]]), array([[0.6769622]]), array([[0.18968584]]), array([[0.16083453]]), array([[0.8852225]]), array([[0.5106319]]), array([[0.7567367]]), array([[0.5418344]]), array([[0.5400608]]), array([[0.4415924]]), array([[0.7417349]]), array([[0.15408535]]), array([[0.47697253]]), array([[0.6099165]]), array([[0.4434947]]), array([[0.6514884]]), array([[0.15338535]]), array([[0.4807239]]), array([[0.3635317]]), array([[0.9112366]]), array([[0.5637028]]), array([[0.4931639]]), array([[0.648393]]), array([[0.895768]]), array([[0.3214459]]), array([[0.6310783]]), array([[0.5204651]]), array([[0.4745777]]), array([[0.4383821]]), array([[0.3456082]]), array([[0.2059386]]), array([[0.516092]]), array([[0.5486792]]), array([[0.628332]]), array([[0.7497101]]), array([[0.6399492]]), array([[0.5535303]]), array([[0.19610676]]), array([[0.8767869]]), array([[0.6533678]]), array([[0.5622378]]), array([[0.3807095]]), array([[0.619389]]), array([[0.3746178]]), array([[0.3776726]]), array([[0.320974]]), array([[0.8622299]]), array([[0.49737]]), array([[0.5533711]]), array([[0.6191195]]), array([[0.6132588]]), array([[0.5198453]]), array([[0.581772]]), array([[0.5637282]]), array([[0.5254159]]), array([[0.8949365]]), array([[0.5237552]]), array([[0.3212434]]), array([[0.15623475]]), array([[0.30655431]]), array([[0.5206385]]), array([[0.909618]]), array([[0.827061]]), array([[0.5380904]]), array([[0.5798516]]), array([[0.6090909]]), array([[0.5101671]]), array([[0.6201181]]), array([[0.6079342]]), array([[0.6655319]]), array([[0.2391182]]), array([[0.5490542]]), array([[0.3846266]]), array([[0.672245]]), array([[0.8870682]]), array([[0.6141648]]), array([[0.5105393]]), array([[0.5509385]]), array([[0.5162206]]), array([[0.6531393]]), array([[0.5445527]]), array([[0.6514884]]), array([[0.5671048]]), array([[0.19106775]]), array([[0.4690769]]), array([[0.5899002]]), array([[0.5355565]]), array([[0.4873479]]), array([[0.4539058]]), array([[0.3725184]]), array([[0.8940886]]), array([[0.24784823]]), array([[0.605946]]), array([[0.5166535]]), array([[0.6533678]]), array([[0.2410539]]), array([[0.3392405]]), array([[0.5218818]]), array([[0.5198021]]), array([[0.5886444]]), array([[0.5227192]]), array([[0.7485747]]), array([[0.5099074]]), array([[0.4821869]]), array([[0.8842358]]), array([[0.4987634]]), array([[0.6673186]]), array([[0.3760074]]), array([[0.7401628]]), array([[0.6167244]]), array([[0.6405612]]), array([[0.2438557]]), array([[0.3753309]]), array([[0.4759203]]), array([[0.760901]]), array([[0.5641722]]), array([[0.6823531]]), array([[0.6257677]]), array([[0.5234096]]), array([[0.7322409]]), array([[0.4120942]]), array([[0.3143922]]), array([[0.309474]]), array([[0.22987793]]), array([[0.5741818]]), array([[0.617985]]), array([[0.8955677]]), array([[0.4458684]]), array([[0.25518139]]), array([[0.4803983]]), array([[0.4826237]]), array([[0.899475]]), array([[0.6736491]]), array([[0.5081632]]), array([[0.18539232]]), array([[0.3692121]]), array([[0.6468475]]), array([[0.903181]]), array([[0.5568043]]), array([[0.6045449]]), array([[0.5784402]]), array([[0.16014724]]), array([[0.4109911]]), array([[0.16843749]]), array([[0.15550247]]), array([[0.3736192]]), array([[0.5258361]]), array([[0.709243]]), array([[0.6072354]]), array([[0.3792624]]), array([[0.7239384]]), array([[0.54574]]), array([[0.9048917]]), array([[0.5145094]]), array([[0.4690214]]), array([[0.5494287]]), array([[0.6528865]]), array([[0.64489387]]), array([[0.5090797]]), array([[0.5014654]]), array([[0.8664585]]), array([[0.7475682]]), array([[0.2105315]]), array([[0.4017995]]), array([[0.1965295]]), array([[0.14800065]]), array([[0.4739299]]), array([[0.5123869]]), array([[0.5618373]]), array([[0.7534728]]), array([[0.5005715]]), array([[0.7497101]]), array([[0.3642562]]), array([[0.3501378]]), array([[0.44828783]]), array([[0.6716283]]), array([[0.4691906]]), array([[0.5807953]]), array([[0.8920211]]), array([[0.8361662]]), array([[0.5460987]]), array([[0.5107031]]), array([[0.8219566]]), array([[0.4990704]]), array([[0.5026399]]), array([[0.5313515]]), array([[0.51075113]]), array([[0.5024457]]), array([[0.4681153]]), array([[0.4492907]]), array([[0.4703913]]), array([[0.6372462]]), array([[0.5428982]]), array([[0.5268256]]), array([[0.5131556]]), array([[0.5812875]]), array([[0.5418009]]), array([[0.4611811]]), array([[0.4853632]]), array([[0.5933172]]), array([[0.54294]]), array([[0.3910227]]), array([[0.632621]]), array([[0.4260666]]), array([[0.24831717]]), array([[0.15700755]]), array([[0.5516636]]), array([[0.33904291]]), array([[0.6512449]]), array([[0.25147049]]), array([[0.6321619]]), array([[0.7428687]]), array([[0.6207721]]), array([[0.3293868]]), array([[0.6181608]]), array([[0.4853994]]), array([[0.4576103]]), array([[0.6025181]]), array([[0.7225265]]), array([[0.7207421]]), array([[0.6287406]]), array([[0.7574302]]), array([[0.6236639]]), array([[0.7207421]]), array([[0.14785305]]), array([[0.8761761]]), array([[0.6037703]]), array([[0.4470645]]), array([[0.7371285]]), array([[0.886206]]), array([[0.5002568]]), array([[0.5020831]]), array([[0.5241717]]), array([[0.7239745]]), array([[0.40063143]]), array([[0.5344883]]), array([[0.3575605]]), array([[0.7466952]]), array([[0.7837579]]), array([[0.5626178]]), array([[0.6208507]]), array([[0.14972025]]), array([[0.4348754]]), array([[0.6207619]]), array([[0.7275499]]), array([[0.5534942]]), array([[0.448633]]), array([[0.5639292]]), array([[0.623]]), array([[0.5652967]]), array([[0.7105087]]), array([[0.4831946]]), array([[0.8952318]]), array([[0.7621838]]), array([[0.3802922]]), array([[0.6575708]]), array([[0.4643526]]), array([[0.22710733]]), array([[0.3777225]]), array([[0.2640735]]), array([[0.4342315]]), array([[0.5513516]]), array([[0.7421307]]), array([[0.7818688]]), array([[0.2980362]]), array([[0.5457633]]), array([[0.4969209]]), array([[0.5327322]]), array([[0.5064025]]), array([[0.7226967]]), array([[0.6862377]]), array([[0.14324607]]), array([[0.5872756]]), array([[0.7091616]]), array([[0.7350813]]), array([[0.4271845]]), array([[0.3096419]]), array([[0.8204059]]), array([[0.719858]]), array([[0.5058624]]), array([[0.2015381]]), array([[0.5626343]]), array([[0.3701939]]), array([[0.7359393]]), array([[0.3822076]]), array([[0.5015729]]), array([[0.2067767]]), array([[0.6033425]]), array([[0.4972975]]), array([[0.7239015]]), array([[0.5363681]]), array([[0.3954847]]), array([[0.2434329]]), array([[0.4523537]]), array([[0.4587453]]), array([[0.14461166]]), array([[0.5808187]]), array([[0.6524302]]), array([[0.16180314]]), array([[0.3738432]]), array([[0.33216813]]), array([[0.6617202]]), array([[0.44802843]]), array([[0.7349725]]), array([[0.3569627]]), array([[0.7106561]]), array([[0.5461278]]), array([[0.5540173]]), array([[0.5663896]]), array([[0.369836]]), array([[0.6758411]]), array([[0.3593881]]), array([[0.5654606]]), array([[0.5095639]]), array([[0.5302916]]), array([[0.6317345]]), array([[0.6181777]]), array([[0.8532545]]), array([[0.24631471]]), array([[0.51117973]]), array([[0.5184629]]), array([[0.5221369]]), array([[0.8936688]]), array([[0.2912192]]), array([[0.2413224]]), array([[0.6202922]]), array([[0.587216]]), array([[0.5133449]]), array([[0.5319686]]), array([[0.8929572]]), array([[0.371189]]), array([[0.3763596]]), array([[0.5869856]]), array([[0.14253164]]), array([[0.5485557]]), array([[0.8222942]])]\n",
      "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.5462842]]), 0.5, array([[0.5113053]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.5729402]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.561869]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.4712991]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.4527197]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.4257713]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.6453767]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.4257713]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.4792185]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.459891]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.5787842]]), 0.5, 0.5, 0.5, array([[0.5364175]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.4702526]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.5461615]]), 0.5, array([[0.5447772]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.5462842]]), 0.5, 0.5, array([[0.4755539]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.4864282]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.5857995]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.4527197]]), 0.5, array([[0.4712991]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.552641]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.6209008]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.7620587]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.7568015]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.8933951]]), 0.5, 0.5, array([[0.8938211]]), array([[0.7585732]]), array([[0.7230043]]), 0.5, 0.5, 0.5, array([[0.7247979]]), 0.5, 0.5, 0.5, array([[0.5225022]]), 0.5, 0.5, array([[0.7556979]]), 0.5, array([[0.7235481]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.5313898]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.7565058]]), array([[0.637109]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.7567367]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.6514884]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.648393]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.7497101]]), array([[0.6399492]]), 0.5, 0.5, 0.5, array([[0.6533678]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.5198453]]), 0.5, 0.5, array([[0.5254159]]), array([[0.8949365]]), 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.909618]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.6531393]]), 0.5, array([[0.6514884]]), 0.5, 0.5, 0.5, 0.5, array([[0.5355565]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.6533678]]), 0.5, 0.5, array([[0.5218818]]), array([[0.5198021]]), 0.5, 0.5, array([[0.7485747]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.8955677]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.903181]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.7239384]]), 0.5, array([[0.9048917]]), 0.5, 0.5, 0.5, array([[0.6528865]]), 0.5, 0.5, 0.5, 0.5, array([[0.7475682]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.7534728]]), 0.5, array([[0.7497101]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.8920211]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.5313515]]), 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.6372462]]), 0.5, array([[0.5268256]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.6512449]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.7225265]]), array([[0.7207421]]), 0.5, array([[0.7574302]]), 0.5, array([[0.7207421]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.5241717]]), array([[0.7239745]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.7275499]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.8952318]]), array([[0.7621838]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.7226967]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.719858]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.7239015]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.6617202]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.5302916]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.8936688]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, array([[0.8929572]]), 0.5, 0.5, 0.5, 0.5, 0.5, 0.5]\n",
      "[array([[0.5811659]]), array([[0.3498842]]), array([[0.6558569]]), array([[0.4512396]]), array([[0.5399966]]), array([[0.2759452]]), array([[0.7196248]]), array([[0.57122383]]), array([[0.4461764]]), array([[0.6361606]]), array([[0.31352]]), array([[0.5779409]]), array([[0.4314772]]), array([[0.302548]]), array([[0.5627253]]), array([[0.6934032]]), array([[0.25512803]]), array([[0.36310465]]), array([[0.3711212]]), array([[0.8756383]]), array([[0.3822727]]), array([[0.8043745]]), array([[0.648995]]), array([[0.3192891]]), array([[0.6514716]]), array([[0.49618773]]), array([[0.3498818]]), array([[0.7091318]]), array([[0.4013474]]), array([[0.5730821]]), array([[0.6606771]]), array([[0.6312027]]), array([[0.5734912]]), array([[0.8754009]]), array([[0.3385542]]), array([[0.2981984]]), array([[0.328216]]), array([[0.6514716]]), array([[0.3499769]]), array([[0.3711212]]), array([[0.680099]]), array([[0.3653515]]), array([[0.508075]]), array([[0.5979178]]), array([[0.5898062]]), array([[0.8636393]]), array([[0.6891247]]), array([[0.3491722]]), array([[0.23979786]]), array([[0.4432317]]), array([[0.5596465]]), array([[0.633173]]), array([[0.7166077]]), array([[0.3769315]]), array([[0.5953022]]), array([[0.5560789]]), array([[0.7877549]]), array([[0.7877549]]), array([[0.2906333]]), array([[0.5167063]]), array([[0.4616213]]), array([[0.8761531]]), array([[0.296525]]), array([[0.7280925]]), array([[0.4009449]]), array([[0.3765438]]), array([[0.49440279]]), array([[0.20350056]]), array([[0.7976223]]), array([[0.6795536]]), array([[0.873136]]), array([[0.5657838]]), array([[0.5934351]]), array([[0.3710365]]), array([[0.5195269]]), array([[0.4139273]]), array([[0.57043973]]), array([[0.4775621]]), array([[0.5988654]]), array([[0.3118927]]), array([[0.3991729]]), array([[0.4053267]]), array([[0.4301429]]), array([[0.7976223]]), array([[0.6656883]]), array([[0.4926199]]), array([[0.20869616]]), array([[0.8387458]]), array([[0.4860743]]), array([[0.4435018]]), array([[0.43393443]]), array([[0.6020079]]), array([[0.5171208]]), array([[0.4691184]]), array([[0.3471371]]), array([[0.5384466]]), array([[0.5690934]]), array([[0.3963159]]), array([[0.8167147]]), array([[0.3336874]]), array([[0.6667299]]), array([[0.48659129]]), array([[0.7802679]]), array([[0.3113683]]), array([[0.6361606]]), array([[0.7684159]]), array([[0.4403065]]), array([[0.7095264]]), array([[0.7731584]]), array([[0.4663619]]), array([[0.8136028]]), array([[0.4928369]]), array([[0.4135078]]), array([[0.6976898]]), array([[0.3896862]]), array([[0.6157124]]), array([[0.713739]]), array([[0.6656883]]), array([[0.597407]]), array([[0.5771655]]), array([[0.45957163]]), array([[0.577675]]), array([[0.3765398]]), array([[0.34460661]]), array([[0.6374507]]), array([[0.55469]]), array([[0.6422522]]), array([[0.712716]]), array([[0.30678347]]), array([[0.6684827]]), array([[0.4809288]]), array([[0.4424228]]), array([[0.4211364]]), array([[0.687024]]), array([[0.23043233]]), array([[0.6492132]]), array([[0.7732703]]), array([[0.5266029]]), array([[0.7976223]]), array([[0.4451513]]), array([[0.6537885]]), array([[0.2728511]]), array([[0.6361606]]), array([[0.3866866]]), array([[0.6810226]]), array([[0.3716535]]), array([[0.6361606]]), array([[0.28549163]]), array([[0.6109333]]), array([[0.2728511]]), array([[0.602255]]), array([[0.3164662]]), array([[0.3698025]]), array([[0.4707785]]), array([[0.8387458]]), array([[0.45209249]]), array([[0.4694513]]), array([[0.5480435]]), array([[0.3416556]]), array([[0.25283983]]), array([[0.4610162]]), array([[0.4564353]]), array([[0.6292358]]), array([[0.674095]]), array([[0.4074111]]), array([[0.873136]]), array([[0.6502494]]), array([[0.2551301]]), array([[0.7976223]]), array([[0.4870233]]), array([[0.5399966]]), array([[0.7166077]]), array([[0.5898062]]), array([[0.4451513]]), array([[0.6312027]]), array([[0.22014583]]), array([[0.7165062]]), array([[0.3113376]]), array([[0.5083417]]), array([[0.7243442]]), array([[0.4615766]]), array([[0.3242432]]), array([[0.6542406]]), array([[0.8929572]]), array([[0.4020365]]), array([[0.3905659]]), array([[0.3149842]]), array([[0.25298353]]), array([[0.6799917]]), array([[0.4396764]]), array([[0.5326271]]), array([[0.4659421]]), array([[0.26526542]]), array([[0.2679729]]), array([[0.3694099]]), array([[0.7976223]]), array([[0.5176575]]), array([[0.6056592]]), array([[0.34133705]]), array([[0.5979178]]), array([[0.7137112]]), array([[0.5638236]]), array([[0.2637592]]), array([[0.4211364]]), array([[0.7238829]]), array([[0.380611]]), array([[0.382708]]), array([[0.3881094]]), array([[0.4279465]]), array([[0.7166077]]), array([[0.5053062]]), array([[0.7032891]]), array([[0.371137]]), array([[0.3293144]]), array([[0.4127244]]), array([[0.2512106]]), array([[0.5747014]]), array([[0.6361606]]), array([[0.3498818]]), array([[0.712716]]), array([[0.3058447]]), array([[0.6279526]]), array([[0.6361606]]), array([[0.5184154]]), array([[0.8072449]]), array([[0.873136]]), array([[0.4022475]]), array([[0.445039]]), array([[0.5585106]]), array([[0.3575676]]), array([[0.1923138]]), array([[0.6361606]]), array([[0.6361606]]), array([[0.873136]]), array([[0.6317145]]), array([[0.8831472]]), array([[0.4301429]]), array([[0.5979178]]), array([[0.50177733]]), array([[0.6674351]]), array([[0.1923138]]), array([[0.30678347]]), array([[0.594001]]), array([[0.7877549]]), array([[0.4013474]]), array([[0.6361606]]), array([[0.7767916]]), array([[0.5426711]]), array([[0.5349411]]), array([[0.602255]]), array([[0.65903923]]), array([[0.6236716]]), array([[0.3814276]]), array([[0.3429304]]), array([[0.6103849]]), array([[0.3090722]]), array([[0.5053062]]), array([[0.597407]]), array([[0.4020365]]), array([[0.7145004]]), array([[0.5259102]]), array([[0.5698252]]), array([[0.4652273]]), array([[0.5594021]]), array([[0.6492132]]), array([[0.3177721]]), array([[0.5109882]]), array([[0.4818539]]), array([[0.5102496]]), array([[0.3991729]]), array([[0.4013474]]), array([[0.35612651]]), array([[0.3113683]]), array([[0.37027659]]), array([[0.2866056]]), array([[0.5219456]]), array([[0.4424228]]), array([[0.6718385]]), array([[0.4211364]]), array([[0.4298698]]), array([[0.529887]]), array([[0.6976219]]), array([[0.3917408]]), array([[0.4159963]]), array([[0.2702027]]), array([[0.8754009]]), array([[0.2866056]]), array([[0.357613]]), array([[0.4279465]]), array([[0.8136028]]), array([[0.4645087]]), array([[0.5052672]]), array([[0.7748213]]), array([[0.7976223]]), array([[0.490119]]), array([[0.6039379]]), array([[0.508075]]), array([[0.4258137]]), array([[0.40022323]]), array([[0.4948654]]), array([[0.5234465]]), array([[0.5405949]]), array([[0.658707]]), array([[0.712716]]), array([[0.4354471]]), array([[0.4279465]]), array([[0.4488406]]), array([[0.4424228]]), array([[0.4127244]]), array([[0.4013474]]), array([[0.5502149]]), array([[0.6765716]]), array([[0.2866056]]), array([[0.6361606]]), array([[0.7011396]]), array([[0.45924643]]), array([[0.5723415]]), array([[0.7196248]]), array([[0.7731584]]), array([[0.523078]]), array([[0.5970449]]), array([[0.22014583]]), array([[0.7731584]]), array([[0.6168118]]), array([[0.4488406]]), array([[0.3581398]]), array([[0.4958761]]), array([[0.6304215]]), array([[0.30389859]]), array([[0.712716]]), array([[0.5088515]]), array([[0.6123218]]), array([[0.6410222]]), array([[0.3885331]]), array([[0.5191587]]), array([[0.3687392]]), array([[0.582133]]), array([[0.7029821]]), array([[0.6674351]]), array([[0.4419837]]), array([[0.5666609]]), array([[0.7615649]]), array([[0.3385542]]), array([[0.4104466]]), array([[0.2944127]]), array([[0.5197354]]), array([[0.4707785]]), array([[0.58333371]]), array([[0.820576]]), array([[0.5268057]]), array([[0.4612016]]), array([[0.2637592]]), array([[0.6328504]]), array([[0.4154881]]), array([[0.712716]]), array([[0.38970421]]), array([[0.2412475]]), array([[0.5267309]]), array([[0.62831]]), array([[0.39285027]]), array([[0.7101008]]), array([[0.58612845]]), array([[0.24818433]]), array([[0.7165062]]), array([[0.3731396]]), array([[0.33965723]]), array([[0.48602643]]), array([[0.58333371]]), array([[0.4546523]]), array([[0.7267077]]), array([[0.4031155]]), array([[0.6667299]]), array([[0.7389696]]), array([[0.8157042]]), array([[0.5357429]]), array([[0.2799694]]), array([[0.6188916]]), array([[0.6361606]]), array([[0.6138127]]), array([[0.3770236]]), array([[0.6810226]]), array([[0.7615649]]), array([[0.5102496]]), array([[0.65903923]]), array([[0.4775703]]), array([[0.435955]]), array([[0.5326271]]), array([[0.4603398]]), array([[0.21499133]]), array([[0.6976898]]), array([[0.4067293]]), array([[0.5619205]]), array([[0.5053062]]), array([[0.594001]]), array([[0.6644284]]), array([[0.7166077]]), array([[0.6799214]]), array([[0.39285027]]), array([[0.6542406]]), array([[0.7615649]])]\n",
      "          A         B         C         D\n",
      "0  0.830019  0.544154  0.779447  0.521819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emilh\\AppData\\Local\\Temp\\ipykernel_14308\\2249937450.py:122: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  c_index = pd.concat([c_index, new_row_df], ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "# Implement and run the requested cross-validation. Report and interpret its results.\n",
    "c_index = scross_validate(x, y, pairs)\n",
    "print(c_index)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A: 0.830019 B: 0.544154 C: 0.766567 D: 0.521819\n",
    "\n",
    "The results show, as was expected, that models trained with in-sample data have a much higher c-index than models trained on partially or completely out-of-sample data. Case D, which represents models where testing data has neither member in common with the test data has the lowest c-index of aroun 52%. This seems to suggest that the chosen model with the chosen hyperparameter (knearestneighbor regression with k = 10), while getting a good score for in-sample data, is bad at generalizing beyond the given sample. The choice for hyperparameter value k = 10 also meant that it wasn't even possible to train out-of-sample models  with some of the testing choices (Mainly cases of B where only the second member is shared but also in some cases of C where data points share the first member with the testing sample), since the dataset only had 400 values and the sklearn library requires a training set of at least 10 samples for each round of the leave-one-out cross validation.\n",
    "I would therefore not recommend continuing with this model, but instead recommend looking for a model with better generalization capability for this specific task, or to test the knearestneighbors regression with a different k-value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
